{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-16T02:41:25.821334Z","iopub.execute_input":"2024-09-16T02:41:25.821601Z","iopub.status.idle":"2024-09-16T02:41:26.186900Z","shell.execute_reply.started":"2024-09-16T02:41:25.821571Z","shell.execute_reply":"2024-09-16T02:41:26.185903Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/pii-detection-removal-from-educational-data/sample_submission.csv\n/kaggle/input/pii-detection-removal-from-educational-data/train.json\n/kaggle/input/pii-detection-removal-from-educational-data/test.json\n","output_type":"stream"}]},{"cell_type":"code","source":"TRAINING_MODEL_PATH = \"microsoft/deberta-v3-base\"\nTRAINING_MAX_LENGTH = 1024\nOUTPUT_DIR = \"output\"","metadata":{"execution":{"iopub.status.busy":"2024-09-16T02:41:26.188497Z","iopub.execute_input":"2024-09-16T02:41:26.188975Z","iopub.status.idle":"2024-09-16T02:41:26.193648Z","shell.execute_reply.started":"2024-09-16T02:41:26.188935Z","shell.execute_reply":"2024-09-16T02:41:26.192541Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!pip install seqeval evaluate -q","metadata":{"execution":{"iopub.status.busy":"2024-09-16T02:41:36.493072Z","iopub.execute_input":"2024-09-16T02:41:36.493838Z","iopub.status.idle":"2024-09-16T02:41:54.640636Z","shell.execute_reply.started":"2024-09-16T02:41:36.493786Z","shell.execute_reply":"2024-09-16T02:41:54.639570Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import json\nimport pandas as pd\nimport numpy as np\nimport argparse\nfrom itertools import chain\nfrom functools import partial\nimport torch\nfrom transformers import AutoTokenizer, Trainer, TrainingArguments\nfrom transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification\nimport evaluate\nfrom datasets import Dataset, features\nfrom seqeval.metrics import recall_score, precision_score\nfrom seqeval.metrics import classification_report\nfrom seqeval.metrics import f1_score","metadata":{"execution":{"iopub.status.busy":"2024-09-16T02:41:54.642967Z","iopub.execute_input":"2024-09-16T02:41:54.643782Z","iopub.status.idle":"2024-09-16T02:42:13.186622Z","shell.execute_reply.started":"2024-09-16T02:41:54.643730Z","shell.execute_reply":"2024-09-16T02:42:13.185801Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_data = json.load(open('/kaggle/input/pii-detection-removal-from-educational-data/train.json'))","metadata":{"execution":{"iopub.status.busy":"2024-09-16T02:42:13.188519Z","iopub.execute_input":"2024-09-16T02:42:13.189471Z","iopub.status.idle":"2024-09-16T02:42:15.732772Z","shell.execute_reply.started":"2024-09-16T02:42:13.189424Z","shell.execute_reply":"2024-09-16T02:42:15.731732Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in train_data]))))\nlabel2id = {l: i for i,l in enumerate(all_labels)}\nid2label = {v:k for k,v in label2id.items()}\n\ntarget = [item for item in all_labels if item != 'O']\n\nprint(id2label)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T02:42:15.734235Z","iopub.execute_input":"2024-09-16T02:42:15.734578Z","iopub.status.idle":"2024-09-16T02:42:15.814312Z","shell.execute_reply.started":"2024-09-16T02:42:15.734543Z","shell.execute_reply":"2024-09-16T02:42:15.813320Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"{0: 'B-EMAIL', 1: 'B-ID_NUM', 2: 'B-NAME_STUDENT', 3: 'B-PHONE_NUM', 4: 'B-STREET_ADDRESS', 5: 'B-URL_PERSONAL', 6: 'B-USERNAME', 7: 'I-ID_NUM', 8: 'I-NAME_STUDENT', 9: 'I-PHONE_NUM', 10: 'I-STREET_ADDRESS', 11: 'I-URL_PERSONAL', 12: 'O'}\n","output_type":"stream"}]},{"cell_type":"code","source":"def rebuild_text(data):\n    \n    text, labels = [], []\n    \n    for tok, lab, ws in zip(\n        data[\"tokens\"], data[\"provided_labels\"], data[\"trailing_whitespace\"]\n    ):\n        # append each token to the reconstructed text and the label for each token's character\n        text.append(tok)\n        labels.extend([lab] * len(tok))\n        \n        # add space in text if whitespace and label \"O\"\n        if ws:\n            text.append(\" \")\n            labels.append(\"O\")\n            \n    return text, labels","metadata":{"execution":{"iopub.status.busy":"2024-09-16T02:42:22.332297Z","iopub.execute_input":"2024-09-16T02:42:22.333168Z","iopub.status.idle":"2024-09-16T02:42:22.338983Z","shell.execute_reply.started":"2024-09-16T02:42:22.333127Z","shell.execute_reply":"2024-09-16T02:42:22.338067Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def tokenize(data, tokenizer, label2id, max_length):\n    \n    text, labels = rebuild_text(data)\n    text = \"\".join(text)\n    labels = np.array(labels)\n    token_labels = []\n    \n    # returns a dictionary-like object containing tokenized inputs and offsets mapping (represents the mapping between the tokens and their corresponding positions in the original text)\n    tokenized = tokenizer(text, return_offsets_mapping=True, max_length=max_length)\n    \n    for start_idx, end_idx in tokenized.offset_mapping:\n        \n        # if CLS tokens\n        if start_idx == 0 and end_idx == 0:\n            token_labels.append(label2id[\"O\"])\n            continue\n            \n        # if token starts with ws\n        if text[start_idx].isspace():\n            start_idx += 1\n            \n        token_labels.append(label2id[labels[start_idx]])\n        \n    length = len(tokenized.input_ids)\n\n    return {**tokenized, \"labels\": token_labels, \"length\": length}","metadata":{"execution":{"iopub.status.busy":"2024-09-16T02:42:32.931391Z","iopub.execute_input":"2024-09-16T02:42:32.932115Z","iopub.status.idle":"2024-09-16T02:42:32.940221Z","shell.execute_reply.started":"2024-09-16T02:42:32.932071Z","shell.execute_reply":"2024-09-16T02:42:32.939175Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(TRAINING_MODEL_PATH)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T02:42:41.715317Z","iopub.execute_input":"2024-09-16T02:42:41.716195Z","iopub.status.idle":"2024-09-16T02:42:44.463072Z","shell.execute_reply.started":"2024-09-16T02:42:41.716151Z","shell.execute_reply":"2024-09-16T02:42:44.462024Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2a32c689dae4262853ebdc8915c59c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cc27f8f20ab4d04a61e22521f9b7139"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3afa6e57e7394d268d9f06e77ae43084"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"ds = Dataset.from_dict({\n    \"full_text\": [x[\"full_text\"] for x in train_data],\n    \"document\": [str(x[\"document\"]) for x in train_data],\n    \"tokens\": [x[\"tokens\"] for x in train_data],\n    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in train_data],\n    \"provided_labels\": [x[\"labels\"] for x in train_data],\n})","metadata":{"execution":{"iopub.status.busy":"2024-09-16T02:42:51.952457Z","iopub.execute_input":"2024-09-16T02:42:51.952851Z","iopub.status.idle":"2024-09-16T02:42:54.086604Z","shell.execute_reply.started":"2024-09-16T02:42:51.952790Z","shell.execute_reply":"2024-09-16T02:42:54.085548Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"ds = ds.map(tokenize, fn_kwargs={\"tokenizer\":tokenizer, \"label2id\":label2id, \"max_length\":TRAINING_MAX_LENGTH}, num_proc=3)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T02:43:02.209045Z","iopub.execute_input":"2024-09-16T02:43:02.210051Z","iopub.status.idle":"2024-09-16T02:43:35.332458Z","shell.execute_reply.started":"2024-09-16T02:43:02.209997Z","shell.execute_reply":"2024-09-16T02:43:35.331567Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=3):   0%|          | 0/6807 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be0dfa75df3b4041b07ec906949520a5"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Compare tokens and labels for original dataset and new tokenization\nx = ds[0]\n\nfor t,l in zip(x[\"tokens\"], x[\"provided_labels\"]):\n    if l != \"O\":\n        print((t,l))\n\nprint(\"*\"*100)\n\nfor t, l in zip(tokenizer.convert_ids_to_tokens(x[\"input_ids\"]), x[\"labels\"]):\n    if id2label[l] != \"O\":\n        print((t,id2label[l]))","metadata":{"execution":{"iopub.status.busy":"2024-09-16T02:43:35.334573Z","iopub.execute_input":"2024-09-16T02:43:35.334904Z","iopub.status.idle":"2024-09-16T02:43:35.350786Z","shell.execute_reply.started":"2024-09-16T02:43:35.334867Z","shell.execute_reply":"2024-09-16T02:43:35.349872Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"('Nathalie', 'B-NAME_STUDENT')\n('Sylla', 'I-NAME_STUDENT')\n('Nathalie', 'B-NAME_STUDENT')\n('Sylla', 'I-NAME_STUDENT')\n('Nathalie', 'B-NAME_STUDENT')\n('Sylla', 'I-NAME_STUDENT')\n****************************************************************************************************\n('N', 'B-NAME_STUDENT')\n('atha', 'B-NAME_STUDENT')\n('lie', 'B-NAME_STUDENT')\n('â–S', 'I-NAME_STUDENT')\n('ylla', 'I-NAME_STUDENT')\n('N', 'B-NAME_STUDENT')\n('atha', 'B-NAME_STUDENT')\n('lie', 'B-NAME_STUDENT')\n('â–S', 'I-NAME_STUDENT')\n('ylla', 'I-NAME_STUDENT')\n('N', 'B-NAME_STUDENT')\n('atha', 'B-NAME_STUDENT')\n('lie', 'B-NAME_STUDENT')\n('â–S', 'I-NAME_STUDENT')\n('ylla', 'I-NAME_STUDENT')\n","output_type":"stream"}]},{"cell_type":"code","source":"def compute_metrics(p, all_labels):\n    # p is a tuple containing preds and true labels\n    predictions, labels = p\n    # preds are in form of probs for each label for each token => we take the highest one\n    predictions = np.argmax(predictions, axis=2)\n\n    # Remove special tokens from preds and labels\n    true_predictions = [\n        [all_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    \n    true_labels = [\n        [all_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    \n    # Compute metrics using sklearn and own formula\n    recall = recall_score(true_labels, true_predictions)\n    precision = precision_score(true_labels, true_predictions)\n    f1_score = (1 + 5*5) * recall * precision / (5*5*precision + recall)\n    \n    # Store metrics and return\n    results = {\n        'recall': recall,\n        'precision': precision,\n        'f1': f1_score\n    }\n    \n    return results","metadata":{"execution":{"iopub.status.busy":"2024-09-16T02:43:35.352042Z","iopub.execute_input":"2024-09-16T02:43:35.352410Z","iopub.status.idle":"2024-09-16T02:43:35.361077Z","shell.execute_reply.started":"2024-09-16T02:43:35.352366Z","shell.execute_reply":"2024-09-16T02:43:35.360182Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForTokenClassification.from_pretrained(\n    TRAINING_MODEL_PATH,\n    num_labels=len(all_labels),\n    id2label=id2label,\n    label2id=label2id,\n    ignore_mismatched_sizes=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T02:43:46.345702Z","iopub.execute_input":"2024-09-16T02:43:46.346126Z","iopub.status.idle":"2024-09-16T02:43:50.624211Z","shell.execute_reply.started":"2024-09-16T02:43:46.346085Z","shell.execute_reply":"2024-09-16T02:43:50.623199Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99bc8faf17b345f299b47050810dd50a"}},"metadata":{}},{"name":"stderr","text":"Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T02:43:57.784570Z","iopub.execute_input":"2024-09-16T02:43:57.785283Z","iopub.status.idle":"2024-09-16T02:43:57.789514Z","shell.execute_reply.started":"2024-09-16T02:43:57.785239Z","shell.execute_reply":"2024-09-16T02:43:57.788655Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Define training arguments\nargs = TrainingArguments(\n    output_dir=OUTPUT_DIR, \n    fp16=True,\n    learning_rate=2e-5,\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=2,\n    report_to=\"none\",\n    evaluation_strategy=\"no\",\n    do_eval=False,\n    save_total_limit=1,\n    logging_steps=20,\n    lr_scheduler_type='cosine',\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    warmup_ratio=0.1,\n    weight_decay=0.01\n)\n\ntrainer = Trainer(\n    model=model, \n    args=args, \n    train_dataset=ds,\n    data_collator=collator, \n    tokenizer=tokenizer,\n    compute_metrics=partial(compute_metrics, all_labels=all_labels),\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T02:44:23.949272Z","iopub.execute_input":"2024-09-16T02:44:23.949676Z","iopub.status.idle":"2024-09-16T02:44:24.431902Z","shell.execute_reply.started":"2024-09-16T02:44:23.949637Z","shell.execute_reply":"2024-09-16T02:44:24.430887Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-09-16T02:44:35.076205Z","iopub.execute_input":"2024-09-16T02:44:35.077177Z","iopub.status.idle":"2024-09-16T03:07:10.241419Z","shell.execute_reply.started":"2024-09-16T02:44:35.077134Z","shell.execute_reply":"2024-09-16T03:07:10.240268Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='425' max='425' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [425/425 22:30, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>1.756100</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.077000</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.006800</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.010200</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.007000</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.003500</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.003100</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.001500</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.004900</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.002300</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.003300</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.002000</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.001900</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.002700</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.001400</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.001600</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.001600</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.001700</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.002000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=425, training_loss=0.08906083535923458, metrics={'train_runtime': 1354.4415, 'train_samples_per_second': 5.026, 'train_steps_per_second': 0.314, 'total_flos': 3406207463440128.0, 'train_loss': 0.08906083535923458, 'epoch': 0.9988249118683902})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.save_model(\"deberta3base_1024\")\ntokenizer.save_pretrained(\"deberta3base_1024\")","metadata":{"execution":{"iopub.status.busy":"2024-09-16T03:07:10.243175Z","iopub.execute_input":"2024-09-16T03:07:10.243487Z","iopub.status.idle":"2024-09-16T03:07:12.024180Z","shell.execute_reply.started":"2024-09-16T03:07:10.243454Z","shell.execute_reply":"2024-09-16T03:07:12.022986Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"('deberta3base_1024/tokenizer_config.json',\n 'deberta3base_1024/special_tokens_map.json',\n 'deberta3base_1024/spm.model',\n 'deberta3base_1024/added_tokens.json',\n 'deberta3base_1024/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}